//The following content is currently being converted to asciidoc format so it can be seamlessly incorporated into the RHOSP docs. It is targeting OSP 13 docs.
:target-upstream:

[id=`overview-of-service-assurance-framework_{context}`]
== Overview of Service Assurance Framework

ifdef::target-downstream[]
This feature is available in this release as a Technology Preview, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see Scope of Coverage Details. 			
endif::[]

Service Assurance Framework (SAF) is an application running on the Red Hat OpenShift Container Platform (OCP). Use SAF to collect metrics and record events from the nodes in your systems that you want to monitor. The metrics and event information travels on a message bus to the server side for storage. Use this centralized information as the source for alerts, visualization, or the source of truth for orchestration frameworks.

[id=`architecture_{context}`]
== Architecture

SAF uses the following components: 
* collectd to collect metrics
* Prometheus as time-series data storage
* An AMQP 1.x compatible messaging bus to shuttle the metrics to SAF for storage in Prometheus
* Smart Gateway

The following diagram is an overview of SAF architecture: 			
On the client side, collectd collects high-resolution metrics. The data is delivered to Prometheus using the AMQP1 plugin, which places the data onto the message bus. On the server side, a Golang application called the Smart Gateway takes the data stream from the bus and exposes it as a local scrape endpoint for Prometheus. 

Server-side SAF monitoring infrastructure consists of the following layers:

* Service Assurance Framework 1.0 (SAF)
* Red Hat OpenShift Container Platform 3.11 (OCP)
* Infrastructure platform 



[id="installation-size_{context}"]
== Sizing your installation

The size of your installation depends on the following factors:
* The number of nodes being monitored
* The number of metrics being collected
* The resolution of metrics
* The length of time for which the data is intended to be stored

The sizing of the virtual machines for Red Hat OpenShift has the largest impact on the hardware requirements, including the number of virtual machines. For more information about the recommended sizing for the OpenShift nodes, see link:https://docs.openshift.com/container-platform/3.11/install/prerequisites.html#production-level-hardware-requirements[Production Level Hardware Requirements].

[id=`prerequisites-for-saf-deployment_{context}`]
= Installing the core components of SAF

This section describes the prerequisites required for a successful SAF installation and describes the installation of the core SAF components. 		

== Prerequisites for SAF Deployment

Complete the following prerequisite tasks:

. Deploy OCP 3.11 and a bastion node, which executes the supplied bash script to load the components into the `sa-telemetry` namespace. If you already have an OCP 3.11 environment, see <<preparing-your-openshift-environment-for-saf_installing-the-core-components-of-saf>>.

. Install a suitable platform on which to install OCP, for example, link:https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/[Red Hat OpenStack Platform]. 

. Install OpenShift Container Platform (OCP). For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/index[OpenShift Container Platform 3.11 Installing Clusters].

[id=`the-core-components-of-saf_{context}`]
== The core components of SAF

When you install SAF, you are loading Kubernetes manifests into OpenShift, with either the `oc` tool or web interface. The manifests set the required state of objects, for example, creating a deployment. You can find the manifests for SAF in the `deploy/` directory of the telemetry-framework release archive.

For a copy of the deployment manifests and installation script, go to
link:https://github.com/redhat-service-assurance/telemetry-framework/releases[https://github.com/redhat-service-assurance/telemetry-framework/releases].

Loading the manifests results in the instantiation of the link:https://coreos.com/blog/introducing-operators.html[Operators] into memory. You can also load additional manifests into memory where the Operators manage the deployment of application components, manage their lifecycle, application configurations, and so on.

SAF has three core components:

* Prometheus (and the AlertManager)
* Smart Gateway
* AMQ Interconnect

Each of these components has a corresponding Operator you can use to load the various application components and objects.

To prepare your environment for SAF, complete the following procedures:

. Prepare your OpenShift environment for SAF. For more information, see <<preparing-your-openshift-environment-for-saf_installing-the-core-components-of-saf>>.

. Create an RHCC Secret. For more information, see <<creating-an-rhcc-secret_installing-the-core-components-of-saf>>.

. Generate a TLS certificate for AMQ Interconnect. For more information, see <<generating-a-tls-certificate-for-amq-interconnect_installing-the-core-components-of-saf>>.

. Deploy SAF to the OpenShift environment. For more information, see <<deploying-saf-to-the-openshift-environment__installing-the-core-components-of-saf>>.

[id=`preparing-your-openshift-environment-for-saf_{context}`]
== Preparing your OpenShift environment for SAF 

Complete the following steps:

. Log into your Red Hat OpenShift environment. Create and switch to the `sa-telemetry` namespace in your OpenShift environment.

----
	oc login https://console.service-assurance.tld:8443
	oc new-project sa-telemetry
----

. Download the latest release file of the telemetry-framework manifests from link:https://github.com/redhat-service-assurance/telemetry-framework/releases[https://github.com/redhat-service-assurance/telemetry-framework/releases]. 

----
	mkdir /<working_directory> ; cd /<working_directory>
	curl --location \
https://github.com/redhat-service-assurance/telemetry-framework/archive/<release_version>.zip -o telemetry-framework.zip
----

. Extract the contents and change to that directory.

----
	unzip telemetry-framework.zip
	cd telemetry-framework-<release_version>
----

ifdef::target-downstream[]
[id="creating-an-rhcc-secret_{context}"]
== Creating an RHCC Secret

To import the applicable container images from the Red Hat Container Catalog (RHCC), you must create an RHCC secret. For more information about getting started with the RHCC, see link:https://access.redhat.com/containers/#/started[Red Hat Container Catalog Get Started Guide].

To create an RHCC secret, complete the following steps:

. Create a registry service account. For more information, see link:https://access.redhat.com/RegistryAuthentication[Red Hat Container Registry Authentication].

. Create a manifest that you can load into OpenShift. This instantiates a secret resource to use for importing the container images from RHCC. Download the `<unique_name>-auth.json` file from the _Docker Configuration_ tab after creating your authentication. Create the following sample manifest for your registry service account in the `sa-telemetry` namespace previously created:

----
	cat > serviceassurance-auth.json.yaml <<EOF
	{
                "auths": {
	        "registry.redhat.io": {
  	            "auth": "NjM0MD..."
	         }
                 }
             }

	EOF
----

. Use the `oc` tool to create the secret resource:

----
	oc create secret generic serviceassurance-pull-secret --from-file=".dockerconfigjson=serviceassurance-auth.json" --type='kubernetes.io/dockerconfigjson'
----
endif::[]

[id=`generating-a-tls-certificate-for-amq-interconnect_{context}`]
== Generating a TLS certificate for AMQ Interconnect 

To get the remote QDR connections through the OpenShift route, use TLS/SSL certificates. The following two commands create the appropriate certificate files locally and load the contents into a secret for use by QDR. The QDR on the client side connects to the route address (DNS address) for the QDR service on port 443.

. Generate an unsigned certificate. If you have a signed certificate to load into Red Hat OpenShift, go to the next step.

----
	openssl req -new -x509 -batch -nodes -days 11000 \
    	-subj "/O=io.interconnectedcloud/CN=qdr-white.sa-telemetry.svc.cluster.local " \
    	-out /tmp/tls.crt \
    	-keyout /tmp/tls.key
----

. Use the `oc` command to import the certificate into Red Hat OpenShift:

----
	oc create secret tls qdr-white-cert \
  	--cert=/tmp/tls.crt \
  	--key=/tmp/tls.key
----

[id=`deploying-saf-to-the-openshift-environment_{context}`]
== Deploying SAF to the OpenShift environment

To install SAF in an OpenShift environment, complete the following tasks: 

. Import the downstream container images into the `sa-telemetry` namespace using the `import-downstream.sh` script. For more information, see <<importing-the-container-images-for-saf_installing-the-core-components-of-saf>>.

. Generate the custom manifests using the Ansible playbook `deploy_builder.yaml` via the `ansible-playbook` command. For more information, see <<generating-the-manifests-for-saf_installing-the-core-components-of-saf>>.

. Execute the `deploy.sh` script to create the Kubernetes objects in the OpenShift environment. For more information, see <<installing-saf-components-using-a-script_installing-the-core-components-of-saf>>.

[id='importing-the-container-images-for-saf_{context}']
=== Importing the container images for SAF 

To import the container images as image streams into OpenShift, run the following commands:

----
cd deploy/
./import-downstream.sh
----

For more information about image streams, see link:https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/builds_and_image_streams.html#image-streams[Builds and Image Streams].

[id=`generating-the-manifests-for-saf_{context}`]
=== Generating the manifests for SAF

Several of the manifests required for deployment are dynamically generated with Ansible. 

[Note:] Ansible version 2.6 or later is recommended.

To generate the additional manifests for SAF, ensure that you are logged into the OCP environment within the `sa-telemetry` namespace and run the following command:

----
ansible-playbook \
-e "registry_path=$(oc registry info)" \
-e "imagestream_namespace=$(oc project --short)" \
deploy_builder.yml
----

By default a persistent volume claim (PVC) of 20G is requested for Prometheus. To adjust the default PVC size, insert -e `“`prometheus_pvc_storage_request=<size_in_gigabytes>G`”` before `deploy_builder.yml` in the command.

[id=`installing-saf-components-using-a-script_{context}`]
=== Installing SAF components using a script

Use the `deploy.sh` script in the `deploy/` directory of the telemetry-framework release file that you previously extracted. Run the script with no arguments (or the `CREATE` argument) to start the various components in your OCP deployment. To remove the components, supply the `DELETE` argument to the script.
Before you run the provided script, ensure that you meet the following prerequisites: 

* You are logged into OCP as an administrator and have the `oc` tool readily available in your `$PATH`. The `deploy.sh` script performs a validation to ensure that this is true. The script switches to the `sa-telemetry` namespace prior to deploying, and if it cannot find that namespace, attempts to create it.

* You have extracted the contents of the telemetry-framework release archive and have changed to the extracted directory. For more information, see <<preparing-your-openshift-environment-for-saf_installing-the-core-components-of-saf>>.
Run the deploy script:
----
./deploy.sh
----

= Completing the SAF configuration

[id=`setting-up-openstack-on-the-client-side_{context}`]
== Setting up OpenStack on the client side
To collect metrics and send them back to the SAF storage domain, you must install collectd and AMQ Interconnect on the nodes of an OpenStack deployment. The following sections show you how to configure Red Hat OpenStack director to enable the data collection functionality and streaming that data back to SAF.

[id=`configuring-red-hat-openstack-platform-overcloud-for-saf_{context}`]
[[configuring-red-hat-openstack-platform-overcloud-for-saf]]
== Configuring Red Hat OpenStack Platform Overcloud for SAF
The following contains a sample from the `metrics-collectd-qdr.yaml` environment file that you can pass to a Red Hat OpenStack 13 director deployment to configure and setup collectd and QDR.

----
---
parameter_defaults:
  CollectdAmqpInstances:
	telemetry:
  	format: JSON
  	presettle: true
  CollectdDefaultPollingInterval: 1
  CollectdConnectionType: amqp1
  CollectdExtraPlugins:
  - cpu
  - df
  - hugepages
  - ovs_events
  - ovs_stats
  - load
  - uptime
  MetricsQdrConnectors:
  - host: qdr-white-port-5671-sa-telemetry.apps.service-assurance.tld
	port: 443
	role: edge
	sslProfile: sslProfile
	verifyHostname: false
  MetricsQdrSSLProfiles:
  - name: sslProfile
----

Create a file and for convenience, name it `metrics-collectd-qdr.yaml` and save it in the `/home/stack/` directory.

By default, the collectd plugins `disk`, `interface`, `load`,` memory`, `processes`, and `tcpconns` are enabled.

To enable additional plugins, use `CollectdExtraPlugins`. It is recommended that you list the default plugins to make it clear which plugins are enabled.

For deployments that use Open vSwitch, add `ovs-stats` to the `CollectdExtraPlugins` list. To monitor the disk usage, add the `df` plugin. 

The `virt` plugin is enabled on overcloud nodes running the `libvirt` service by default. The following example plugin configuration, added to `metrics-collectd-qdr.yaml`, is for the `virt` plugin:

----
ExtraConfig:
	collectd::plugin::virt::connection: "qemu:///system"
	collectd::plugin::virt::hostname_format: "hostname uuid"
----

Use the `metrics-collectd-qdr.yaml` file to configure the plugins for collectd, including the `amqp1.so` module to connect to AMQ Interconnect. Use the `CollectdExtraPlugins` parameter to enable additional plugins. Use the `MetricsQdrConnectors` parameter to configure the connection back to the SAF server where data is streamed for storage in the appropriate storage backend provided by SAF.

[id=`updating-red-hat-openstack-platform-overcloud-for-saf_{context}`]
[[updating-red-hat-openstack-platform-overcloud-for-saf]]
== Updating Red Hat OpenStack Platform Overcloud for SAF
Below is an example `openstack overcloud deploy` command with the `metrics-collectd-qdr.yaml` environment file that you configured in the previous section. Note the two environment file lines that you must provide in the deploy command.

----
openstack overcloud deploy \
--timeout 100 \
--templates /usr/share/openstack-tripleo-heat-templates \
--stack overcloud \
--libvirt-type kvm \
--ntp-server 192.168.1.254 \
-e /home/stack/virt/config_lvm.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
-e /home/stack/virt/network/network-environment.yaml \
-e /home/stack/virt/hostnames.yml \
-e /home/stack/virt/debug.yaml \
-e /home/stack/virt/nodes_data.yaml \
--environment-file /usr/share/openstack-tripleo-heat-templates/environments/metrics-collectd-qdr.yaml \
-e /home/stack/virt/metrics-qdr-collectd.yaml \
-e /home/stack/virt/docker-images.yaml \
--log-file overcloud_deployment_42.log
----

[NOTE:] 
The SSL certificates for the `MetricsQdr` service is configured to generate only for the `InternalApi` network but the default Ceph role does not use the `InternalApi` network. To deploy SAF client when InternalTLS is enabled, use this workaround: pass the custom Ceph role, which has `InternalApi` network, to `openstack overcloud deploy` when InternalTLS is enabled in the deployment.

[id=`completion of the server-side installation_{context}`]
== Completion of the server-side installation
The Service Assurance Framework provides a high-resolution, low latency framework for streaming metrics back to a centralized data store. In the future, you can add additional data components such as events and logging.

ifdef::target-downstream[]
SAF is currently in technical preview for the Red Hat OpenStack Platform 13 and requires a support exception to allow for deployments to be supported. If SAF looks useful for your environment, contact your Technical Account Manager.
endif::[]

= Configuring SAF data collection

Now that the SAF server-side components have been implemented and are ready for store the data collected on the cloud platform side, we need to enable data collection within your OpenStack environment and direct that data back to the SAF as deployed above.

In the next sections we will configure the collectd (metrics and events data collector) and the AMQ Interconnect (message bus) so that data collection from your OpenStack cloud can be stored into the SAF application.

== Data Collecting agent 

Performance monitoring collects system information periodically and provides a mechanism to store and monitor the values in a variety of ways using a data collecting agent. Red Hat supports the collectd daemon as a collection agent. This daemon stores the data in a time-series database. One of the Red Hat supported databases is called Prometheus. You can use this stored data to monitor systems, find performance bottlenecks, and predict future system load.

[Note:] Red Hat OpenStack Platform supports performance monitoring only on the client side.
 
== Installing collectd

To install collectd on the overcloud, complete the following steps: 
. Copy the file /usr/share/openstack-tripleo-heat-templates/environments/collectd-environment.yaml to your local directory.  Open the file, set the following parameters, and list the plugins you want under CollectdExtraPlugins. You can also provide parameters in the ExtraConfig section: 	
----
parameter_defaults:
   CollectdExtraPlugins:
     - disk
     - df
     - virt

   ExtraConfig:
     collectd::plugin::virt::connection: "qemu:///system"
     collectd::plugin::virt::hostname_format: "hostname uuid"
----

By default, collectd comes with the disk, interface, load, memory, processes, and tcpconns plugins. You can add additional plugins using the `CollectdExtraPlugins` parameter. You can also provide additional configuration information for the `CollectdExtraPlugins` using the `ExtraConfig` option as shown. The example above adds the virt plugin and configures the connection string and the hostname format. 	

. Include the modified YAML files in the `openstack overcloud deploy` command to install the collectd daemon on all overcloud nodes. For example: 	
	
----
$ openstack overcloud deploy 
--templates /home/templates/environments/collectd.yaml \
-e /path-to-copied/collectd-environment.yaml
----

To view the collectd plugins and configurations, see Appendix A: collectd plugins and configurations.

= Configuring SAF for multi-cloud

Multiple Openstack clouds can be configured to target a single instance of SAF. 
There are a few steps to get this set up:

. Plan the AMQP address prefixes to use for each cloud
. Deploy metrics and events consumer Smart Gateways for each cloud to listen on
  the corresponding address prefixes
. Configure each cloud to send it's metrics and events to SAF on the
  correct address

== AMQP addresses

By default, OpenStack nodes are configured to send data to the 
`collectd/telemetry` and `collectd/notify` addresses on the AMQP bus; and SAF is
configured to listen on those addresses for monitoring data. In order to support
multiple clouds and have the ability to easily identify which cloud generated
which monitoring data, each cloud should be configured to send to a unique
address.

It is recommended to prefix a cloud identifier to the second part of the
address. For example:

* collectd/cloud1-telemetry
* collectd/cloud1-notify
* collectd/cloud2-telemetry
* collectd/cloud2-notify
* collectd/us-east-1-telemetry
* collectd/us-west-3-telemetry
* ...etc

== Deploying Smart Gateways

Two Smart Gateways (one for metrics, one for events) need to be deployed
for each cloud, configured to listen on the correct AMQP address. For example:

----
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud1-telemetry
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud1-telemetry
  serviceType: metrics

---
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud1-notify
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud1-notify
  serviceType: events

---
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud2-telemetry
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud2-telemetry
  serviceType: metrics

---
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud2-notify
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud2-notify
  serviceType: events
----


== Openstack configuration

In order to label traffic according to it's cloud of origin, the collectd
configuration has to be updated to have cloud-specific instance names. This is
usually accomplished by editing your Openstack director configuration to have 
the following `CollectdAmqpInstances`.

metrics-collectd-qdr.yaml
----
parameter_defaults:
    CollectdAmqpInstances:
        cloud1-telemetry:
            format: JSON
            presettle: false
        cloud1-notify:
            notify: true
            format: JSON
            presettle: true
----

See <<configuring-red-hat-openstack-platform-overcloud-for-saf>> and <<updating-red-hat-openstack-platform-overcloud-for-saf>> above for details of how to edit and redeploy this configuration.

== Querying metrics data from multiple clouds

Data in prometheus will have a "service" label attached according to which
smartgateway it was scraped from, so this label can be used to query data from a
specific cloud; for example: `sa_collectd_uptime{service="cloud1-smartgateway"}`

[id=`conclusion_{context}`]
== Conclusion
The Service Assurance Framework provides a high-resolution, low latency framework for streaming metrics back to a centralized data store. In the future, you can add additional data components such as events and logging.

SAF is currently in technical preview for the Red Hat OpenStack Platform 13 and requires a support exception to allow for deployments to be supported. If SAF looks useful for your environment, contact your Technical Account Manager.
