//The following content is currently being converted to asciidoc format so it can be seamlessly incorporated into the RHOSP docs. It is targeting OSP 13 docs.
:target-upstream:
:imagesdir: images/

[id=`overview-of-service-assurance-framework_{context}`]
== Overview of Service Assurance Framework

ifdef::target-downstream[]
This feature is available in this release as a Technology Preview, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see Scope of Coverage Details. 			
endif::[]

Service Assurance Framework (SAF) is an application running on the Red Hat OpenShift Container Platform (OCP). Use SAF to collect metrics and record events from the nodes in your systems that you want to monitor. The metrics and event information travels on a message bus to the server side for storage. Use this centralized information as the source for alerts, visualization, or the source of truth for orchestration frameworks.

[id=`architecture_{context}`]
== Architecture

SAF uses the following components:

* collectd to collect metrics
* Prometheus as time-series data storage
* ElasticSearch as events data storage
* An AMQP 1.x compatible messaging bus to shuttle the metrics and events to SAF
for storage in Prometheus and ElasticSearch respectively
* Smart Gateway

The following diagram is an overview of SAF architecture:

#TODO: Update diagram to include events architecture#

image::SAF_Overview_37_0819_arch.png[SAF Architecture Diagram]

On the client side, collectd collects high-resolution metrics. The data is delivered to Prometheus using the AMQP1 plugin, which places the data onto the message bus. On the server side, a Golang application called the Smart Gateway takes the data stream from the bus and exposes it as a local scrape endpoint for Prometheus. 

Server-side SAF monitoring infrastructure consists of the following layers:

* Service Assurance Framework 1.x (SAF)
* Red Hat OpenShift Container Platform 3.11 (OCP)
* Infrastructure platform 

image::SAF_Overview_37_0819_deployment_prereq.png[SAF Server-side Infrastructure Diagram]

[id="installation-size_{context}"]
== Sizing your installation

The size of your installation depends on the following factors:

* The number of nodes being monitored
* The number of metrics and events being collected
* The resolution of metrics
* The length of time for which the data is intended to be stored

The sizing of the virtual machines for Red Hat OpenShift has the largest impact on the hardware requirements, including the number of virtual machines. For more information about the recommended sizing for the OpenShift nodes, see link:https://docs.openshift.com/container-platform/3.11/install/prerequisites.html#production-level-hardware-requirements[Production Level Hardware Requirements].

[id=`prerequisites-for-saf-deployment_{context}`]
= Installing the core components of SAF

This section describes the prerequisites required for a successful SAF installation and describes the installation of the core SAF components. 		

== Prerequisites for SAF Deployment

Complete the following prerequisite tasks:

. Deploy OCP 3.11 and a bastion node, which executes the supplied bash script to load the components into the `sa-telemetry` namespace. If you already have an OCP 3.11 environment, see <<preparing-your-openshift-environment-for-saf_installing-the-core-components-of-saf>>.

. Install a suitable platform on which to install OCP, for example, link:https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/[Red Hat OpenStack Platform]. 

. Install OpenShift Container Platform (OCP). For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/index[OpenShift Container Platform 3.11 Installing Clusters].

[id=`the-core-components-of-saf_{context}`]
== The core components of SAF

When you install SAF, you are loading Kubernetes manifests into OpenShift, with either the `oc` tool or web interface. The manifests set the desired state of objects, for example, creating a deployment. You can find the manifests for SAF in the `deploy/` directory of the telemetry-framework release archive.

For a copy of the deployment manifests and installation script, go to
link:https://github.com/redhat-service-assurance/telemetry-framework/releases[https://github.com/redhat-service-assurance/telemetry-framework/releases].

Loading the manifests results in the instantiation of the link:https://coreos.com/blog/introducing-operators.html[Operators] into memory. You can also load additional manifests into memory where the Operators manage the deployment of application components, manage their lifecycle, application configurations, and so on.

SAF has these core components:

* Prometheus (and the AlertManager)
* ElasticSearch
* Smart Gateway
* AMQ Interconnect

Each of these components has a corresponding Operator you can use to load the various application components and objects.

To prepare your environment for SAF, complete the following procedures:

. Prepare your OpenShift environment for SAF. For more information, see <<preparing-your-openshift-environment-for-saf_installing-the-core-components-of-saf>>.

. Create an RHCC Secret. For more information, see <<creating-an-rhcc-secret_installing-the-core-components-of-saf>>.

. Generate a TLS certificate for AMQ Interconnect. For more information, see
<<generating-a-tls-certificate-for-amq-interconnect_installing-the-core-components-of-saf>>.

. Generate TLS certificates for ElasticSearch. For more information, see
<<generating-tls-certificates-for-elasticsearch_installing-the-core-components-of-saf>>.

. Deploy SAF to the OpenShift environment. For more information, see <<deploying-saf-to-the-openshift-environment__installing-the-core-components-of-saf>>.

[id=`preparing-your-openshift-environment-for-saf_{context}`]
[[preparing-your-openshift-environment-for-saf]]
== Preparing your OpenShift environment for SAF 

Complete the following steps:

. Log into your Red Hat OpenShift environment. Create and switch to the `sa-telemetry` namespace in your OpenShift environment.

[source,bash]
----
	oc login https://console.service-assurance.tld:8443
	oc new-project sa-telemetry
----

. Download the latest release file of the telemetry-framework manifests from link:https://github.com/redhat-service-assurance/telemetry-framework/releases[https://github.com/redhat-service-assurance/telemetry-framework/releases]. 

[source,bash]
----
	mkdir /<working_directory> ; cd /<working_directory>
	curl --location \
https://github.com/redhat-service-assurance/telemetry-framework/archive/<release_version>.zip -o telemetry-framework.zip
----

. Extract the contents and change to that directory.

[source,bash]
----
	unzip telemetry-framework.zip
	cd telemetry-framework-<release_version>
----

. The cluster administrator must set `vm.max_map_count` sysctl to `262144` on
the host level of each node in your cluster prior to instantiating an SAF
instance if ElasticSearch is intended to be deployed.

ifdef::target-upstream[]
[id="quickstart-script-for-development-usage_{context}"]
== Quickstart Script for Development Usage

A script is provided to deploy SAF into an existing
OpenShift environment. It allows for SAF to be started for development
purposes, and is not intended for production environments. It takes care of all
of the required steps for a single-cloud setup.

The script can be found here: https://github.com/redhat-service-assurance/telemetry-framework/blob/master/deploy/quickstart.sh
endif::[]

ifdef::target-downstream[]
[id="creating-an-rhcc-secret_{context}"]
== Creating an RHCC Secret

To import the applicable container images from the Red Hat Container Catalog (RHCC), you must create an RHCC secret. For more information about getting started with the RHCC, see link:https://access.redhat.com/containers/#/started[Red Hat Container Catalog Get Started Guide].

To create an RHCC secret, complete the following steps:

. Create a registry service account. For more information, see link:https://access.redhat.com/RegistryAuthentication[Red Hat Container Registry Authentication].

. Create a manifest that you can load into OpenShift. This instantiates a secret resource to use for importing the container images from RHCC. Download the `<unique_name>-auth.json` file from the _Docker Configuration_ tab after creating your authentication. Create the following sample manifest for your registry service account in the `sa-telemetry` namespace previously created:

[source,json]
----
  cat > serviceassurance-auth.json <<EOF
  {
    "auths": {
      "registry.redhat.io": {
        "auth": "NjM0MD..."
      }
    }
  }
	EOF
----

. Use the `oc` tool to create the secret resource:

[source,bash]
----
	oc create secret generic serviceassurance-pull-secret --from-file=".dockerconfigjson=serviceassurance-auth.json" --type='kubernetes.io/dockerconfigjson'
----
endif::[]

[id=`generating-a-tls-certificate-for-amq-interconnect_{context}`]
== Generating a TLS certificate for AMQ Interconnect 

To get the remote QDR connections through the OpenShift route, use TLS/SSL certificates. The following two commands create the appropriate certificate files locally and load the contents into a secret for use by QDR. The QDR on the client side connects to the route address (DNS address) for the QDR service on port 443.

. Generate an unsigned certificate. If you have a signed certificate to load into Red Hat OpenShift, go to the next step.

[source,bash]
----
	openssl req -new -x509 -batch -nodes -days 11000 \
    	-subj "/O=io.interconnectedcloud/CN=qdr-white.sa-telemetry.svc.cluster.local " \
    	-out /tmp/tls.crt \
    	-keyout /tmp/tls.key
----

. Use the `oc` command to import the certificate into Red Hat OpenShift:

[source,bash]
----
	oc create secret tls qdr-white-cert \
  	--cert=/tmp/tls.crt \
  	--key=/tmp/tls.key
----

[id=`generating-tls-certificates-for-elasticsearch_{context}`]
== Generating TLS certificates for ElasticSearch

To connect the Smart Gateway to ElasticSearch for events storage certificates
need to be generated and loaded as a secret into OpenShift.

. Generate the unsigned certificates. If you have signed certificates to load
into Red Hat OpenShift, go to the next step.

[source,bash]
----
  WORKING_DIR=./es-certs \  # <1>
  NAMESPACE="sa-telemetry" \  # <2>
  ./cert_generation.sh # <3>
----
<1> Directory where the certificates will be stored
<2> OpenShift namespace where the certificates will be loaded. Also used for
hostname generation.
<3> Script located in the `deploy/` directory to help with building appropriate
certificate, CA, and key files.

[id=`deploying-saf-to-the-openshift-environment_{context}`]
== Deploying SAF to the OpenShift environment

To install SAF in an OpenShift environment, complete the following tasks: 

. Import the downstream container images into the `sa-telemetry` namespace using the `import-downstream.sh` script. For more information, see <<importing-the-container-images-for-saf_installing-the-core-components-of-saf>>.

. Generate the custom manifests using the Ansible playbook `deploy_builder.yaml` via the `ansible-playbook` command. For more information, see <<generating-the-manifests-for-saf_installing-the-core-components-of-saf>>.

. Execute the `deploy.sh` script to create the Kubernetes objects in the OpenShift environment. For more information, see <<installing-saf-components-using-a-script_installing-the-core-components-of-saf>>.

[id='importing-the-container-images-for-saf_{context}']
=== Importing the container images for SAF 

To import the container images as image streams into OpenShift, run the following commands:

[source,bash]
----
cd deploy/
./import-downstream.sh
----

For more information about image streams, see link:https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/builds_and_image_streams.html#image-streams[Builds and Image Streams].

ifdef::target-upstream[]
[id='container-versions_{context}']
==== Container Versions

When deploying SAF you can choose to use container images from either upstream
(public open source), or downstream (published to the Red Hat Container Catalog).
Two scripts,`import-downstream.sh` and `import-upstream.sh` are provided to help
with this. In the scripts we attempt to hook into specific versions for
deployment from the source registry. When importing those into our ImageStream
source for delivery of the images from the internal OpenShift registry, we often
use `latest` where possible. In certain instances an Operator or container
artifact may require a specific version format, and thus is reflected in the
container image tag imported into the internal registry.

In the future we hope to better align the versions across the various
ImageStreams and to build a more consistent view between the deployment
methods. It's possible our issues will be resolved with the migration to the
Operator Lifecycle Manager as well.
endif::[]

[id=`generating-the-manifests-for-saf_{context}`]
=== Generating the manifests for SAF

Several of the manifests required for deployment are dynamically generated with Ansible. 

NOTE: Ansible version 2.6 or later is recommended.

To generate the additional manifests for SAF, ensure that you are logged into the OCP environment within the `sa-telemetry` namespace and run the following command:

[source,bash]
----
ansible-playbook \
-e "registry_path=$(oc registry info)" \
-e "imagestream_namespace=$(oc project --short)" \
deploy_builder.yml
----

By default a persistent volume claim (PVC) of 20G is requested for Prometheus. To adjust the default PVC size, insert `-e "prometheus_pvc_storage_request=<size_in_gigabytes>G"` before `deploy_builder.yml` in the command.

[id=`installing-saf-components-using-a-script_{context}`]
=== Installing SAF components using a script

image::SAF_Overview_37_0819_deployment_manually.png[SAF Deployment Diagram]

Use the `deploy.sh` script in the `deploy/` directory of the telemetry-framework release file that you previously extracted. Run the script with no arguments (or the `CREATE` argument) to start the various components in your OCP deployment. To remove the components, supply the `DELETE` argument to the script.
Before you run the provided script, ensure that you meet the following prerequisites: 

* You are logged into OCP as an administrator and have the `oc` tool readily available in your `$PATH`. The `deploy.sh` script performs a validation to ensure that this is true. The script switches to the `sa-telemetry` namespace prior to deploying, and if it cannot find that namespace, attempts to create it.

* You have extracted the contents of the telemetry-framework release archive and have changed to the extracted directory. For more information, see <<preparing-your-openshift-environment-for-saf_installing-the-core-components-of-saf>>.

Run the deploy script:

[source,bash]
----
./deploy.sh
----

= Completing the SAF configuration

[id=`setting-up-openstack-on-the-client-side_{context}`]
== Setting up OpenStack on the client side
To collect metrics and/or events and send them back to the SAF storage domain, you must install collectd and AMQ Interconnect on the nodes of an OpenStack deployment. The following sections show you how to configure Red Hat OpenStack director to enable the data collection functionality and streaming that data back to SAF.

[id=`configuring-red-hat-openstack-platform-overcloud-for-saf_{context}`]
[[configuring-red-hat-openstack-platform-overcloud-for-saf]]
== Configuring Red Hat OpenStack Platform Overcloud for SAF Metrics
The following contains a sample from the `metrics-collectd-qdr-overrides.yaml` environment file that you can pass to a Red Hat OpenStack 13 director deployment to configure and setup collectd and QDR.

.metrics-collectd-qdr-overrides.yaml
[source,yaml]
----
---
parameter_defaults:
  CollectdAmqpInstances:
	telemetry:
  	format: JSON
  	presettle: true
  CollectdDefaultPollingInterval: 1
  CollectdConnectionType: amqp1
  CollectdExtraPlugins:
  - cpu
  - df
  - hugepages
  - ovs_stats
  - load
  - uptime
  MetricsQdrConnectors:
  - host: qdr-white-port-5671-sa-telemetry.apps.service-assurance.tld
	port: 443
	role: edge
	sslProfile: sslProfile
	verifyHostname: false
  MetricsQdrSSLProfiles:
  - name: sslProfile
----

Create a file and for convenience, name it `metrics-collectd-qdr-overrides.yaml` and save it in the `/home/stack/` directory.

By default, the collectd plugins `disk`, `interface`, `load`,` memory`, `processes`, and `tcpconns` are enabled.

To enable additional plugins, use `CollectdExtraPlugins`. It is recommended that you list the default plugins to make it clear which plugins are enabled.

For deployments that use Open vSwitch, add `ovs-stats` to the `CollectdExtraPlugins` list. To monitor the disk usage, add the `df` plugin. 

The `virt` plugin is enabled on overcloud nodes running the `libvirt` service by default. The following example plugin configuration, added to `metrics-collectd-qdr-overrides.yaml`, is for the `virt` plugin:

[source,yaml]
----
ExtraConfig:
	collectd::plugin::virt::connection: "qemu:///system"
	collectd::plugin::virt::hostname_format: "hostname uuid"
----

Use the `metrics-collectd-qdr-overrides.yaml` file to configure the plugins for collectd, including the `amqp1.so` module to connect to AMQ Interconnect. Use the `CollectdExtraPlugins` parameter to enable additional plugins. Use the `MetricsQdrConnectors` parameter to configure the connection back to the SAF server where data is streamed for storage in the appropriate storage backend provided by SAF.


[id=`configuring-red-hat-openstack-platform-overcloud-for-saf-events_{context}`]
[[configuring-red-hat-openstack-platform-overcloud-for-saf-events]]
== Configuring Red Hat OpenStack Platform Overcloud for SAF Events

Enabling the collection of events on the client side is similar to how we
configured metrics in
<<configuring-red-hat-openstack-platform-overcloud-for-saf>>. The following
contains a sample from the `metrics-collectd-qdr-overrides.yaml` environment file that
you can pass to a Red Hat OpenStack 13 director deployment to configure and
setup collectd and QDR.

.metrics-collectd-qdr-overrides.yaml
[source,yaml]
----
---
parameter_defaults:
  CollectdAmqpInstances:
  notify: // <1>
    format: JSON // <2>
    presettle: false
    notify: true // <3>
  telemetry:
    format: JSON
    presettle: true
  CollectdDefaultPollingInterval: 1
  CollectdConnectionType: amqp1
  CollectdExtraPlugins:
  - cpu
  - df
  - hugepages
  - ovs_stats
  - ovs_events // <4>
  - connectivity // <5>
  - load
  - uptime
  MetricsQdrConnectors:
  - host: qdr-white-port-5671-sa-telemetry.apps.service-assurance.tld
	port: 443
	role: edge
	sslProfile: sslProfile
	verifyHostname: false
  MetricsQdrSSLProfiles:
  - name: sslProfile
----
<1> Create a `notify` address for QDR that will transmit the events
<2> Format of the messages is in JSON
<3> Mark these messages as being events in the VES format
<4> Enable an events plugin for OVS events
<5> Enable an events plugin for network connectivity events

[id=`updating-red-hat-openstack-platform-overcloud-for-saf_{context}`]
[[updating-red-hat-openstack-platform-overcloud-for-saf]]
== Updating Red Hat OpenStack Platform Overcloud for SAF
Below is an example `openstack overcloud deploy` command with the `metrics-collectd-qdr-overrides.yaml` environment file that you configured in the previous section. Note the two environment file lines that you must provide in the deploy command.

[source,bash]
----
openstack overcloud deploy \
--timeout 100 \
--templates /usr/share/openstack-tripleo-heat-templates \
--stack overcloud \
--libvirt-type kvm \
--ntp-server 192.168.1.254 \
-e /home/stack/virt/config_lvm.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
-e /home/stack/virt/network/network-environment.yaml \
-e /home/stack/virt/hostnames.yml \
-e /home/stack/virt/debug.yaml \
-e /home/stack/virt/nodes_data.yaml \
--environment-file /usr/share/openstack-tripleo-heat-templates/environments/metrics-collectd-qdr.yaml \
-e /home/stack/virt/metrics-collectd-qdr-overrides.yaml \
-e /home/stack/virt/docker-images.yaml \
--log-file overcloud_deployment_42.log
----

NOTE: The SSL certificates for the `MetricsQdr` service is configured to generate
only for the `InternalApi` network but the default Ceph role does not use the
`InternalApi` network. To deploy SAF client when InternalTLS is enabled, use
this workaround: pass the custom Ceph role, which has `InternalApi` network, to
`openstack overcloud deploy` when InternalTLS is enabled in the deployment.

[id=`completion of the server-side installation_{context}`]
== Completion of the server-side installation
The Service Assurance Framework provides a high-resolution, low latency framework for streaming metrics back to a centralized data store. In the future, you can add additional data components such as events and logging.


= Configuring SAF data collection

Now that the SAF server-side components have been implemented and are ready for store the data collected on the cloud platform side, we need to enable data collection within your OpenStack environment and direct that data back to the SAF as deployed above.

In the next sections we will configure the collectd (metrics and events data collector) and the AMQ Interconnect (message bus) so that data collection from your OpenStack cloud can be stored into the SAF application.

== Data Collecting agent 

Performance monitoring collects system information periodically and provides a mechanism to store and monitor the values in a variety of ways using a data collecting agent. Red Hat supports the collectd daemon as a collection agent. This daemon stores the data in a time-series database. One of the Red Hat supported databases is called Prometheus. You can use this stored data to monitor systems, find performance bottlenecks, and predict future system load.

[Note:] Red Hat OpenStack Platform supports performance monitoring only on the client side.
 
== Installing collectd

To install collectd on the overcloud, complete the following steps: 

. Copy the file
/usr/share/openstack-tripleo-heat-templates/environments/collectd-environment.yaml
to your local directory.  Open the file, set the following parameters, and list
the plugins you want under CollectdExtraPlugins. You can also provide
parameters in the ExtraConfig section:

[source,yaml]
----
parameter_defaults:
   CollectdExtraPlugins:
     - disk
     - df
     - virt

   ExtraConfig:
     collectd::plugin::virt::connection: "qemu:///system"
     collectd::plugin::virt::hostname_format: "hostname uuid"
----

By default, collectd comes with the disk, interface, load, memory, processes, and tcpconns plugins. You can add additional plugins using the `CollectdExtraPlugins` parameter. You can also provide additional configuration information for the `CollectdExtraPlugins` using the `ExtraConfig` option as shown. The example above adds the virt plugin and configures the connection string and the hostname format. 	

. Include the modified YAML files in the `openstack overcloud deploy` command to install the collectd daemon on all overcloud nodes. For example: 	

[source,bash]
----
$ openstack overcloud deploy 
--templates /home/templates/environments/collectd.yaml \
-e /path-to-copied/collectd-environment.yaml
----

To view the collectd plugins and configurations, see Appendix A: collectd plugins and configurations.

= Configuring SAF for multi-cloud

Multiple OpenStack clouds can be configured to target a single instance of SAF. 
There are a few steps to get this set up:

. Plan the AMQP address prefixes to use for each cloud
. Deploy metrics and events consumer Smart Gateways for each cloud to listen on
  the corresponding address prefixes
. Configure each cloud to send it's metrics and events to SAF on the
  correct address

image::OpenStack SAF Multi-Cloud.png[SAF Multi-Cloud Architecture Diagram]

== AMQP addresses

By default, OpenStack nodes are configured to send data to the 
`collectd/telemetry` and `collectd/notify` addresses on the AMQP bus; and SAF is
configured to listen on those addresses for monitoring data. In order to support
multiple clouds and have the ability to easily identify which cloud generated
which monitoring data, each cloud should be configured to send to a unique
address.

It is recommended to prefix a cloud identifier to the second part of the
address. For example:

* collectd/cloud1-telemetry
* collectd/cloud1-notify
* collectd/cloud2-telemetry
* collectd/cloud2-notify
* collectd/us-east-1-telemetry
* collectd/us-west-3-telemetry
* ...etc

== Deploying Smart Gateways

Two Smart Gateways (one for metrics, one for events) need to be deployed
for each cloud, configured to listen on the correct AMQP address. For example:

[source,yaml]
----
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud1-telemetry
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud1-telemetry
  serviceType: metrics

---
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud1-notify
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud1-notify
  serviceType: events

---
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud2-telemetry
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud2-telemetry
  serviceType: metrics

---
apiVersion: smartgateway.infra.watch/v1alpha1
kind: SmartGateway
metadata:
  name: cloud2-notify
spec:
  amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/cloud2-notify
  serviceType: events
----

== OpenStack configuration

In order to label traffic according to it's cloud of origin, the collectd
configuration has to be updated to have cloud-specific instance names. This is
usually accomplished by editing your OpenStack director configuration to have 
the following `CollectdAmqpInstances`.

.metrics-collectd-qdr-overrides.yaml
[source,yaml]
----
parameter_defaults:
    CollectdAmqpInstances:
        cloud1-telemetry:
            format: JSON
            presettle: true
        cloud1-notify:
            notify: true
            format: JSON
            presettle: false
----

See <<configuring-red-hat-openstack-platform-overcloud-for-saf>> and <<updating-red-hat-openstack-platform-overcloud-for-saf>> above for details of how to edit and redeploy this configuration.

== Querying metrics data from multiple clouds

Data in prometheus will have a "service" label attached according to which
smartgateway it was scraped from, so this label can be used to query data from a
specific cloud; for example: `sa_collectd_uptime{service="cloud1-smartgateway"}`

[id=`conclusion_{context}`]
== Conclusion
The Service Assurance Framework provides a high-resolution, low latency framework for streaming metrics back to a centralized data store. In the future, you can add additional data components such as events and logging.

ifdef::target-downstream[]
SAF is currently in technical preview for the Red Hat OpenStack Platform 13 and requires a support exception to allow for deployments to be supported. If SAF looks useful for your environment, contact your Technical Account Manager.
endif::[]

ifdef::target-upstream[]
= Legal Notice
The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attributionâ€“Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at http://creativecommons.org/licenses/by-sa/3.0/. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version. (https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/service_assurance_framework/index)
endif::[]
